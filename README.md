# lf_tokenizer: another way to tokenize Vietnamese
| | |
| --- | --- |
| Testing |  |
| Package |  |
| Meta | [License - MIT](LICENSE) |

## What is it?

**lf_tokenizer** is a Python package that provides another method to tokenize Vietnamese. 

## Table of Contents

- [Main Features](#main-features)
- [Where to get it](#where-to-get-it)
- [Dependencies](#dependencies)
- [Installation from sources](#installation-from-sources)
- [License](#license)
- [Documentation](#documentation)
<!-- - [Background](#background)
- [Getting Help](#getting-help)
- [Discussion and Development](#discussion-and-development)
- [Contributing to lf_tokenize](#contributing-to-lf_tokenize) -->

## Main Features

## Where to get it
The source code is currently hosted on GitHub at:
https://github.com/justinnguyendsa/fl_tokenizer

```sh
# Git clone (Jupyter Notebook)
!git clone https://github.com/justinnguyendsa/fl_tokenizer
```

## Dependencies

## Installation from sources

## License
[MIT](LICENSE)

## Documentaion

<!-- ## Background

## Getting help

## Discussion and Development

## Contributing to lf_tokenize -->

[Go to Top](#table-of-contents)
