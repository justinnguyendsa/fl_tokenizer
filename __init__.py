from .configs import *

# print(f'Start at: {datetime.now()}')
# stw_org = read_txt('./stopwords/vietnamese-stopwords - reviewed.txt')
# brand_org = read_txt('./stopwords/brand_name_stopwords.txt')

# stw_ls = stw_org.split('\n')
# brand_ls = brand_org.split('\n')
# stw_ls.extend(brand_ls)

# sample_all = read_txt('sample.txt').split('\n')
# sample = sample_all[:20000]

# final_ls = fl_tokenizer(doc=sample, stopwords_ls=stw_ls, threshold=0.01)

# print(final_ls[:100])

# print(f'End at: {datetime.now()}')